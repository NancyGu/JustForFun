{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset ...\n",
      "summary: 36 documents in 2 categories.\n",
      "Loading dataset ...\n",
      "summary: 36 documents in 2 categories.\n",
      "CPU times: user 4.42 ms, sys: 6.29 ms, total: 10.7 ms\n",
      "Wall time: 18.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# naivebayes 垃圾邮件分类器\n",
    "\n",
    "# dataset：英文邮件\n",
    "train_path = \"/Users/gunanxi/PycharmProjects/JustForFun/LightGBM/build/LightGBM/python-package/LSTM_learn/MachineLearning/email/train1\"\n",
    "test_path = \"/Users/gunanxi/PycharmProjects/JustForFun/LightGBM/build/LightGBM/python-package/LSTM_learn/MachineLearning/email/test1\"\n",
    "     \n",
    "def load_data(folder_path):\n",
    "    print(\"Loading dataset ...\")\n",
    "    datalist = datasets.load_files(folder_path)\n",
    "    print(\"summary: {0} documents in {1} categories.\".format(len(datalist.data),len(datalist.target_names)))\n",
    "    return datalist\n",
    "train = load_data(train_path)\n",
    "test = load_data(test_path)\n",
    "\n",
    "def email_num(path,pos):\n",
    "    list = os.listdir(path + '/' + pos)\n",
    "    return len(list)\n",
    "# train集中：正常邮件的数目 和 垃圾邮件的数目\n",
    "normal = email_num(train_path,\"pos\")\n",
    "spam = email_num(train_path,\"neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import ssl #关闭SSL检查，可以解决无法download nltk.stopwords的问题\n",
    " \n",
    "#try:\n",
    "#    _create_unverified_https_context = ssl._create_unverified_context\n",
    "#except AttributeError:\n",
    "#    pass\n",
    "#else:\n",
    "#    ssl._create_default_https_context = _create_unverified_https_context\n",
    " \n",
    "#nltk.download()\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNum: 1888\n",
      "27537.0\n"
     ]
    }
   ],
   "source": [
    "# nltk 做停用词的小栗子\n",
    "word_sum = 0\n",
    "def dropStopwordsAndToken(word_bag,essaydata,word_sum):\n",
    "    # word_bag:dict\n",
    "    data = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\'-]+|[+——！，。？、~@#￥%……&*（）<>]+\", \" \", essaydata)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(data)\n",
    "\n",
    "    word_list = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "    for w in word_list: \n",
    "        if w not in stop_words:\n",
    "            w = w.encode(\"utf-8\") #全部转utf-8编码\n",
    "            w.lower()             #全部转小写\n",
    "            if w in word_bag:\n",
    "                word_bag[w] = word_bag[w] + 1\n",
    "            else:\n",
    "                word_bag[w] = 1.0\n",
    "            word_sum = word_sum + word_bag[w]\n",
    "    return word_bag,word_sum\n",
    "# testing \n",
    "#example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "#word_bag = dropStopwordsAndToken(example_sent)\n",
    "#print(word_bag) \n",
    "word_bag = {}\n",
    "for s in train.data:\n",
    "    s = s.decode('utf-8')\n",
    "    [word_bag,word_sum] = dropStopwordsAndToken(word_bag,s,word_sum)\n",
    "print(\"WordNum:\",len(word_bag))\n",
    "print(word_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1063151.3150133952 462695.16403066204\n",
      "A_preProbablity: [0.5, 0.5]\n",
      "Bi_preProbability: 1889\n",
      "n_PABi: 1090\n",
      "s_PABi: 1120\n"
     ]
    }
   ],
   "source": [
    "#计算先验概率和条件概率\n",
    "normal = email_num(train_path,\"pos\")\n",
    "spam = email_num(train_path,\"neg\")\n",
    "\n",
    "# 正常邮件和垃圾邮件的先验概率\n",
    "def A_pre_probabilty(data):\n",
    "    s_pre_pro = []\n",
    "    #垃圾邮件的先验概率\n",
    "    P_spam = spam /len(data)\n",
    "    s_pre_pro.append(P_spam)\n",
    "    #正常邮件的先验概率\n",
    "    P_normal = normal/len(data)\n",
    "    s_pre_pro.append(P_normal)\n",
    "    \n",
    "    #返回先验概率的列表\n",
    "    return s_pre_pro\n",
    "\n",
    "def Bi_pre_probabilty(word_bag,word_sum):\n",
    "    for n,w in enumerate(word_bag):\n",
    "        if( n == 0):\n",
    "            word_bag.pop(w)\n",
    "        word_bag[w] = 1.0 * n/word_sum\n",
    "    return word_bag\n",
    "\n",
    "\n",
    "#计算每个词在正常邮件垃圾邮件中的数目\n",
    "def wordNum_email(email_repre,wordDic):\n",
    "    #用二维向量存储\n",
    "    num_word = np.zeros((2,len(wordDic)),dtype= np.int)\n",
    "    for i in range(len(wordDic)):\n",
    "        #在正常邮件的数目\n",
    "        for j in range(normal):\n",
    "            num_word[0][i] += email_repre[j][i]\n",
    "        #在垃圾邮件中的数目\n",
    "        for j in range(normal, spam+normal):\n",
    "            num_word[1][i] += email_repre[j][i]\n",
    "    print(num_word)\n",
    "    return num_word\n",
    "\n",
    "#条件概率\n",
    "for index in range(len(train.data)):\n",
    "    s = train.data[index]\n",
    "    s = s.decode('utf-8')\n",
    "    if train.target[index] == 0:\n",
    "        # neg\n",
    "        [s_word_bag,s_word_sum] = dropStopwordsAndToken(s_word_bag,s,s_word_sum)\n",
    "    else:\n",
    "        [n_word_bag,n_word_sum] = dropStopwordsAndToken(n_word_bag,s,n_word_sum)\n",
    "print(s_word_sum,n_word_sum)\n",
    "\n",
    "A_prePro = []\n",
    "if(len(train.data)==0):\n",
    "    print(\"No doc!\")\n",
    "else:\n",
    "    A_prePro = A_pre_probabilty(train.data)\n",
    "    \n",
    "if(word_sum==0):\n",
    "    print(\"No words!\")\n",
    "else:\n",
    "    Bi_prePro = Bi_pre_probabilty(word_bag,word_sum)  \n",
    "\n",
    "n_PABi = Bi_pre_probabilty(n_word_bag,n_word_sum)\n",
    "\n",
    "s_PABi = Bi_pre_probabilty(s_word_bag,s_word_sum)\n",
    "print(\"A_preProbablity:\",A_prePro) #0=,1\n",
    "print(\"Bi_preProbability:\",len(Bi_prePro))\n",
    "print(\"n_PABi:\",len(n_PABi))\n",
    "print(\"s_PABi:\",len(s_PABi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-176.09352385959556 0\n",
      "0 1 1\n",
      "-169.45059427986462 0\n",
      "1 1 1\n",
      "17.415065168640556 0\n",
      "2 0 0\n",
      "4.6459366469453816 0\n",
      "3 1 0\n",
      "5.428462058433862 0\n",
      "4 1 0\n",
      "10.695851836897392 0\n",
      "5 0 0\n",
      "17.18213082454029 0\n",
      "6 0 0\n",
      "3.7899559086914367 0\n",
      "7 0 0\n",
      "21.71797081976376 0\n",
      "8 0 0\n",
      "26.95882202157036 0\n",
      "9 1 0\n",
      "5.965413803004148 0\n",
      "10 1 0\n",
      "26.65478769258722 0\n",
      "11 1 0\n",
      "9.039752819959864 0\n",
      "12 1 0\n",
      "16.599067862070044 0\n",
      "13 1 0\n",
      "8.367696570209475 0\n",
      "14 1 0\n",
      "3.7080124605251 0\n",
      "15 0 0\n",
      "-293.6175622592603 0\n",
      "16 0 1\n",
      "7.070053868439077 0\n",
      "17 0 0\n",
      "5.108938061784813 0\n",
      "18 0 0\n",
      "-6.483248438595487 0\n",
      "19 0 1\n",
      "9.990858138423441 0\n",
      "20 0 0\n",
      "-0.8439917719417773 0\n",
      "21 1 1\n",
      "33.28286486735888 0\n",
      "22 0 0\n",
      "13.909564999480562 0\n",
      "23 0 0\n",
      "21.955696470066247 0\n",
      "24 1 0\n",
      "27.64943722800174 0\n",
      "25 1 0\n",
      "22.628587063645636 0\n",
      "26 0 0\n",
      "-6.208007825609752 0\n",
      "27 1 1\n",
      "6.2432535081744716 0\n",
      "28 0 0\n",
      "4.47997954236388 0\n",
      "29 1 0\n",
      "15.697731749218356 0\n",
      "30 1 0\n",
      "7.399737668697235 0\n",
      "31 1 0\n",
      "8.763432751341206 0\n",
      "32 0 0\n",
      "5.277443452295082 0\n",
      "33 1 0\n",
      "13.100867431176075 0\n",
      "34 0 0\n",
      "8.766352787591734 0\n",
      "35 0 0\n",
      "20 36\n",
      "test accuracy: 0.5555555555555556\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "test_sum = len(test.data)\n",
    "def predict(doc):\n",
    "    doc = doc.decode('utf-8')\n",
    "    test_word_bag,test_word_sum = {},0\n",
    "    [test_word_bag,test_word_sum] = dropStopwordsAndToken(test_word_bag,doc,test_word_sum)\n",
    "    sps,nps = 0,0\n",
    "    \n",
    "    # 是垃圾邮件的概率\n",
    "    for (n,w) in enumerate(test_word_bag):\n",
    "        s = 0\n",
    "        if w in s_PABi and w not in n_PABi:\n",
    "            s = s + s_PABi[w]*3\n",
    "        if w in s_PABi and w in n_PABi:\n",
    "            s = s + (s_PABi[w] - n_PABi[w])*5\n",
    "        if w not in s_PABi and w in n_PABi:\n",
    "            s = s - n_PABi[w]*3\n",
    "        if w in Bi_prePro:\n",
    "            s = s / Bi_prePro[w]\n",
    "        sps = sps + s\n",
    "    # 是正常邮件的概率\n",
    "    for (n,w) in enumerate(test_word_bag):\n",
    "        n = 0\n",
    "        if w in n_PABi and w not in s_PABi:\n",
    "            n = n + n_PABi[w]*3\n",
    "        if w in n_PABi and w in s_PABi:\n",
    "            n = n + (n_PABi[w] - s_PABi[w])*5\n",
    "        if w in s_PABi and w not in n_PABi:\n",
    "            n = n - s_PABi[w]*3\n",
    "        if w in Bi_prePro:\n",
    "            n = n / Bi_prePro[w]\n",
    "        n = nps + n\n",
    "            \n",
    "    print(sps,nps)\n",
    "    if sps>nps:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1  # 返回1:n=pos 0:s=neg\n",
    "\n",
    "acc = 0\n",
    "for index in range(len(test.data)):\n",
    "    target = test.target[index]\n",
    "    pre = predict(test.data[index])\n",
    "    print(index,target,pre)\n",
    "    if( target == pre ):\n",
    "        acc = acc + 1\n",
    "print(acc,len(test.data))\n",
    "print(\"test accuracy:\",acc*1.0/len(test.data))\n",
    "\n",
    "\n",
    "y_pred = [0, 2, 1, 3]\n",
    "y_true = [0, 1, 2, 3]\n",
    "print(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
